{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9788033,"sourceType":"datasetVersion","datasetId":5995943},{"sourceId":9836024,"sourceType":"datasetVersion","datasetId":6033435}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Function to extract MFCC features from audio files\ndef extract_features(audio_file):\n    try:\n        # Load audio file\n        audio, sample_rate = librosa.load(audio_file, sr=None)\n        \n        # Extract MFCCs\n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n        return mfccs.T  # Transpose to get time-steps as rows\n    except Exception as e:\n        print(f\"Error processing {audio_file}: {e}\")\n        return None  # Return None for failed cases\n\n# Load dataset\ndata = pd.read_csv('/kaggle/input/annotated-data2/CSV_file.csv')  # Adjust path if necessary\n\nfeatures = []\nlabels = []\n\n# Directory where audio files are stored\naudio_directory = '/kaggle/input/hindi-hate-speech-audio-data-wav/Converted_Audio_Data/Converted_Audio_Data/'  # Adjust this path as needed\n\n# Extract features for each audio file\nfor index, row in data.iterrows():\n    audio_file_path = os.path.join(audio_directory, row['Name'])\n#     print(audio_file_path)\n    mfccs = extract_features(audio_file_path)\n    \n    if mfccs is not None:\n        features.append(mfccs)\n        labels.append(row['Label'])\n    else:\n        print(f\"Failed to process: {audio_file_path}\")\n\n# Convert to numpy arrays and pad sequences to the same length\nX = pad_sequences(features, padding='post', dtype='float32')\ny = np.array(labels)\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\nnum_classes = len(np.unique(y_encoded))\ny_encoded = np.eye(num_classes)[y_encoded]  # One-hot encoding\n\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y_encoded: {y_encoded.shape}\")\n\n# Train/test split\nif len(X) > 0 and len(y) > 0:\n    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.4, random_state=42)\n\n    print(f\"Shape of X_train: {X_train.shape}\")\n    print(f\"Shape of y_train: {y_train.shape}\")\n    print(f\"Shape of X_test: {X_test.shape}\")\n    print(f\"Shape of y_test: {y_test.shape}\")\n\n    # Build the RCNN model\n    input_shape = X_train.shape[1:]  # Shape (time_steps, n_mfcc)\n    model = Sequential()\n    # Convolutional layers\n    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(MaxPooling1D(pool_size=2))\n    \n    # Optional second Conv1D layer (can be commented out if not needed)\n    model.add(Conv1D(128, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling1D(pool_size=2))\n    \n    # Recurrent (LSTM) layer\n    model.add(LSTM(64, return_sequences=False))\n    model.add(Dropout(0.5))\n    \n    # Fully connected layers\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Train the model\n    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n\n    # Evaluate the model\n    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test accuracy: {test_accuracy}\")\nelse:\n    print(\"No valid features or labels to train the model.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\n\n# Function to extract MFCC features from audio files\ndef extract_features(audio_file):\n    try:\n        # Load audio file with downsampling\n        audio, sample_rate = librosa.load(audio_file)  # Downsampling to 16kHz\n        \n        # Extract MFCCs with fewer coefficients\n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)  # Reduced MFCCs\n        return mfccs.T  # Transpose to get time-steps as rows\n    except Exception as e:\n        print(f\"Error processing {audio_file}: {e}\")\n        return None  # Return None for failed cases\n\n# Load dataset\ndata = pd.read_csv('/kaggle/input/annotated-data2/CSV_file.csv')  # Adjust path if necessary\n\nfeatures = []\nlabels = []\n\n# Directory where audio files are stored\naudio_directory = '/kaggle/input/hindi-hate-speech-audio-data-wav/Converted_Audio_Data/Converted_Audio_Data/'  # Adjust this path as needed\n\n# Extract features for each audio file\nfor index, row in data.iterrows():\n    audio_file_path = os.path.join(audio_directory, row['Name'])\n    mfccs = extract_features(audio_file_path)\n    \n    if mfccs is not None:\n        features.append(mfccs)\n        labels.append(row['Label'])\n    else:\n        print(f\"Failed to process: {audio_file_path}\")\n\n# Convert to numpy arrays and pad sequences to the same length\nX = pad_sequences(features, padding='post', dtype='float32', maxlen=200)  # Limit padding length to 200\ny = np.array(labels)\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\nnum_classes = len(np.unique(y_encoded))\ny_encoded = np.eye(num_classes)[y_encoded]  # One-hot encoding\n\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y_encoded: {y_encoded.shape}\")\n\n# Train/test split\nif len(X) > 0 and len(y) > 0:\n    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.4, random_state=42)\n\n    print(f\"Shape of X_train: {X_train.shape}\")\n    print(f\"Shape of y_train: {y_train.shape}\")\n    print(f\"Shape of X_test: {X_test.shape}\")\n    print(f\"Shape of y_test: {y_test.shape}\")\n\n    # Set up a mirrored strategy for distributed training\n    strategy = tf.distribute.MirroredStrategy(devices=[\"/GPU:0\", \"/GPU:1\"])  # T4x2 GPUs on Kaggle\n    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n\n    with strategy.scope():\n    # Build the WaveNet model\n        input_shape = X_train.shape[1:]  # Shape (time_steps, n_mfcc)\n        model = Sequential()\n\n        # WaveNet-inspired dilated convolutions\n        model.add(Conv1D(64, kernel_size=2, dilation_rate=2, activation='relu', input_shape=input_shape))\n        model.add(Conv1D(128, kernel_size=2, dilation_rate=4, activation='relu'))\n        model.add(Conv1D(256, kernel_size=2, dilation_rate=8, activation='relu'))\n        model.add(Conv1D(512, kernel_size=2, dilation_rate=16, activation='relu'))\n\n        # Flatten the output from the Conv1D layers before passing to dense layers\n        model.add(Flatten())\n\n        # Dense layers\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        # Compile the model\n        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\n    model.fit(X_train, y_train, epochs=5, batch_size=8, validation_data=(X_test, y_test))  # Smaller batch size\n\n# Evaluate the model\n    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:21:00.346288Z","iopub.execute_input":"2024-11-07T18:21:00.347125Z","iopub.status.idle":"2024-11-07T18:28:13.643317Z","shell.execute_reply.started":"2024-11-07T18:21:00.347083Z","shell.execute_reply":"2024-11-07T18:28:13.642298Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Shape of X: (10120, 200, 40)\nShape of y_encoded: (10120, 2)\nShape of X_train: (6072, 200, 40)\nShape of y_train: (6072, 2)\nShape of X_test: (4048, 200, 40)\nShape of y_test: (4048, 2)\nNumber of devices: 2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.6239 - loss: 7.1155 - val_accuracy: 0.6423 - val_loss: 0.6537\nEpoch 2/5\n\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.6675 - loss: 0.6510 - val_accuracy: 0.6423 - val_loss: 0.6535\nEpoch 3/5\n\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.6610 - loss: 0.6550 - val_accuracy: 0.6423 - val_loss: 0.6525\nEpoch 4/5\n\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.6714 - loss: 0.6341 - val_accuracy: 0.6423 - val_loss: 0.6535\nEpoch 5/5\n\u001b[1m759/759\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.6683 - loss: 0.6747 - val_accuracy: 0.6423 - val_loss: 0.6527\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6711 - loss: 0.6347\nTest accuracy: 0.6724308133125305\n","output_type":"stream"}]},{"cell_type":"code","source":"from pydub import AudioSegment\nimport numpy as np\nimport librosa\nimport tensorflow as tf\n\n# Load your audio file\naudio_file_path = r'/kaggle/input/hindi-hate-speech-audio-data-wav/Converted_Audio_Data/Converted_Audio_Data/Audio_3009.wav'\naudio = AudioSegment.from_wav(audio_file_path)\n\n# Define the segment duration and the list of timestamps to mute\nsegment_duration = 500  # Duration of each segment in milliseconds (e.g., 500ms)\nbad_word_indices = []  # This will contain the indices of segments predicted as abusive\n\n# Function to extract features from audio segments\ndef extract_features_from_segment(segment):\n    try:\n        # Convert the AudioSegment to numpy array\n        samples = np.array(segment.get_array_of_samples()).astype(np.float32)\n        # Extract MFCCs (same settings as used in training, using 40 MFCCs for example)\n        mfccs = librosa.feature.mfcc(y=samples, sr=segment.frame_rate, n_mfcc=40)  # Use 40 MFCCs\n        return mfccs.T  # Return the MFCCs for each time-step (transposed)\n    except Exception as e:\n        print(f\"Error processing segment: {e}\")\n        return None\n\n# Split the audio into segments and process them\nsegments = []\nfor i in range(0, len(audio), segment_duration):\n    segment = audio[i:i + segment_duration]\n    mfccs = extract_features_from_segment(segment)\n    \n    if mfccs is not None:\n        segments.append(mfccs)\n\n# Convert list of segments to a numpy array and ensure consistent shape by padding/truncating\nmax_sequence_length = 5349  # Define the length that matches your model's expected input size\nX_test = []\n\n# Ensure all MFCCs have the same length (pad or truncate each segment)\nfor mfcc in segments:\n    if mfcc.shape[0] < max_sequence_length:\n        # Padding with zeros if MFCC sequence is shorter than max_sequence_length\n        padded_mfcc = np.pad(mfcc, ((0, max_sequence_length - mfcc.shape[0]), (0, 0)), mode='constant')\n    else:\n        # Truncate if the MFCC sequence is longer than max_sequence_length\n        padded_mfcc = mfcc[:max_sequence_length, :]\n    X_test.append(padded_mfcc)\n\n# Convert the list to a numpy array\nX_test = np.array(X_test)\n\n# Ensure the input has shape (num_samples, sequence_length, num_features)\n# Here we're using 40 MFCC features per time-step\nprint(f\"X_test shape before reshaping: {X_test.shape}\")\n\n# Ensure the input has the shape (batch_size, time_steps, num_features)\n# Here we're using 40 MFCC features per time-step\n# Reshape if necessary\nif X_test.shape[-1] != 40:\n    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 40)  # Ensure it has 40 features per timestep\n\nprint(f\"X_test shape after reshaping: {X_test.shape}\")\n\n# Now let's check the model's expected output shape before flattening\n# If the error persists, check the actual shape of your model output\ntry:\n    # This will give us an idea of the shape before it is passed to the Dense layers\n    intermediate_output = model.layers[0].output  # First layer to check\n    print(f\"Intermediate output shape before Dense layers: {intermediate_output.shape}\")\n\n    # Make predictions on the audio segments\n    predictions = model.predict(X_test)\n\n    # Debugging predictions\n    print(f\"Predictions: {predictions}\")\n\n    for i, pred in enumerate(predictions):\n        print(f\"Segment {i} prediction: {np.argmax(pred)}\")  # Debugging prediction for each segment\n\n        if np.argmax(pred) == 1:  # Check if the segment is predicted as abusive\n            start_time = i * segment_duration  # Start time in milliseconds\n            end_time = start_time + segment_duration  # End time in milliseconds\n            bad_word_indices.append((start_time, end_time))\n    \n    print(f\"Bad word indices: {bad_word_indices}\")  # Debugging the identified bad words\n\n    # Mute the identified segments\n    for start, end in bad_word_indices:\n        audio = audio[:start] + AudioSegment.silent(duration=end - start) + audio[end:]\n\n    # Save the modified audio\n    audio.export(\"muted_audio_for_3009_500ms_audio.wav\", format=\"wav\")\n\nexcept Exception as e:\n    print(f\"Error during prediction: {e}\")\nelse:\n    print(\"No valid segments to predict.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-07T18:30:46.185390Z","iopub.execute_input":"2024-11-07T18:30:46.186202Z","iopub.status.idle":"2024-11-07T18:30:48.535181Z","shell.execute_reply.started":"2024-11-07T18:30:46.186160Z","shell.execute_reply":"2024-11-07T18:30:48.534161Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"X_test shape before reshaping: (163, 5349, 40)\nX_test shape after reshaping: (163, 5349, 40)\nIntermediate output shape before Dense layers: (None, 198, 64)\nError during prediction: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_6\" is incompatible with the layer: expected axis -1 of input shape to have value 87040, but received input with shape (16, 2723328)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(16, 5349, 40), dtype=float32)\n  • training=False\n  • mask=None\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\n\n# Function to extract MFCC features from audio files\ndef extract_features(audio_file):\n    try:\n        # Load audio file\n        audio, sample_rate = librosa.load(audio_file, sr=None)\n        \n        # Extract MFCCs\n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=15)\n        mfccs = np.mean(mfccs.T, axis=0)  # Take mean of MFCCs over time\n        return mfccs\n    except Exception as e:\n        print(f\"Error processing {audio_file}: {e}\")\n        return None  # Return None for failed cases\n\n# Load your dataset of audio files and labels\ndata = pd.read_csv('/kaggle/input/annotated-data2/CSV_file.csv') # Ensure this path is correct\ndf = pd.DataFrame(data)# Adjust path if necessary\n\nfeatures = []\nlabels = []\n\n# Directory where audio files are stored\naudio_directory = '/kaggle/input/hindi-hate-speech-audio-data-wav/Converted_Audio_Data/Converted_Audio_Data/' # Adjust this\n\nfor index, row in df.iterrows():\n    audio_file_path = os.path.join(audio_directory, row['Name'])  # Use the correct directory\n    mfccs = extract_features(audio_file_path)  # Extract features\n    \n    if mfccs is not None:  # Only append if mfccs are valid\n        features.append(mfccs)  # Append extracted features\n        labels.append(row['Label'])  # Append the label for the current row\n    else:\n        print(f\"Failed to extract features for {audio_file_path}\")\n\n# Convert to numpy arrays\nX = np.array(features)\ny = np.array(labels)\n\n\n# Encode the labels (if they're not already in binary format)\nunique_labels = np.unique(y)  # Get unique labels (e.g., 0 and 1)\nnum_classes = len(unique_labels)  # Number of classes (should be 2 for binary)\ny_encoded = np.eye(num_classes)[y]  # Create one-hot encoded labels\n\n# Print shapes of the labels after encoding\nprint(f\"Shape of y (before encoding): {y.shape}\")\nprint(f\"Shape of y_encoded (after encoding): {y_encoded.shape}\")\n\n# Split the dataset into training and testing sets if features exist\nif len(X) > 0 and len(y) > 0:\n    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.4, random_state=42)\n    \n    # Print shapes after splitting\n    print(f\"Shape of X_train: {X_train.shape}\")\n    print(f\"Shape of y_train: {y_train.shape}\")\n    print(f\"Shape of X_test: {X_test.shape}\")\n    print(f\"Shape of y_test: {y_test.shape}\")\n\n    # Build the model\n    model = Sequential()\n    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer\n    model.add(Dropout(0.5))  # Dropout layer for regularization\n    model.add(Dense(64, activation='relu'))  # Hidden layer\n    model.add(Dropout(0.5))  # Dropout layer for regularization\n    model.add(Dense(num_classes, activation='softmax'))  # Output layer (binary classification)\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Train the model\n    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n\n    # Evaluate the model\n    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test accuracy: {test_accuracy}\")\nelse:\n    print(\"No valid features or labels to train the model.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T07:00:01.103973Z","iopub.execute_input":"2024-11-08T07:00:01.105285Z","iopub.status.idle":"2024-11-08T07:04:10.799133Z","shell.execute_reply.started":"2024-11-08T07:00:01.105219Z","shell.execute_reply":"2024-11-08T07:04:10.798207Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Shape of y (before encoding): (10120,)\nShape of y_encoded (after encoding): (10120, 2)\nShape of X_train: (6072, 15)\nShape of y_train: (6072, 2)\nShape of X_test: (4048, 15)\nShape of y_test: (4048, 2)\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.5386 - loss: 15.2979 - val_accuracy: 0.6700 - val_loss: 0.6495\nEpoch 2/5\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6038 - loss: 1.2080 - val_accuracy: 0.6700 - val_loss: 0.6440\nEpoch 3/5\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6223 - loss: 0.7912 - val_accuracy: 0.6700 - val_loss: 0.6476\nEpoch 4/5\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6448 - loss: 0.6955 - val_accuracy: 0.6700 - val_loss: 0.6379\nEpoch 5/5\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6630 - loss: 0.6793 - val_accuracy: 0.6700 - val_loss: 0.6392\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6711 - loss: 0.6385\nTest accuracy: 0.6699604988098145\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Function to extract MFCC features from audio files\ndef extract_features(audio_file):\n    try:\n        # Load audio file\n        audio, sample_rate = librosa.load(audio_file, sr=None)\n        \n        # Extract MFCCs\n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=20)  # Increased to 20 MFCCs\n        mfccs = np.mean(mfccs.T, axis=0)  # Take mean of MFCCs over time\n        return mfccs\n    except Exception as e:\n        print(f\"Error processing {audio_file}: {e}\")\n        return None  # Return None for failed cases\n\n# Load your dataset of audio files and labels\ndata = pd.read_csv('/kaggle/input/annotated-data2/CSV_file.csv')  # Ensure this path is correct\ndf = pd.DataFrame(data)  # Adjust path if necessary\n\nfeatures = []\nlabels = []\n\n# Directory where audio files are stored\naudio_directory = '/kaggle/input/hindi-hate-speech-audio-data-wav/Converted_Audio_Data/Converted_Audio_Data/'  # Adjust this\n\nfor index, row in df.iterrows():\n    audio_file_path = os.path.join(audio_directory, row['Name'])  # Use the correct directory\n    mfccs = extract_features(audio_file_path)  # Extract features\n    \n    if mfccs is not None:  # Only append if mfccs are valid\n        features.append(mfccs)  # Append extracted features\n        labels.append(row['Label'])  # Append the label for the current row\n    else:\n        print(f\"Failed to extract features for {audio_file_path}\")\n\n# Convert to numpy arrays\nX = np.array(features)\ny = np.array(labels)\n\n# Encode the labels (if they're not already in binary format)\nunique_labels = np.unique(y)  # Get unique labels (e.g., 0 and 1)\nnum_classes = len(unique_labels)  # Number of classes (should be 2 for binary)\ny_encoded = np.eye(num_classes)[y]  # Create one-hot encoded labels\n\n# Print shapes of the labels after encoding\nprint(f\"Shape of y (before encoding): {y.shape}\")\nprint(f\"Shape of y_encoded (after encoding): {y_encoded.shape}\")\n\n# Split the dataset into training and testing sets if features exist\nif len(X) > 0 and len(y) > 0:\n    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.4, random_state=42)\n    \n    # Print shapes after splitting\n    print(f\"Shape of X_train: {X_train.shape}\")\n    print(f\"Shape of y_train: {y_train.shape}\")\n    print(f\"Shape of X_test: {X_test.shape}\")\n    print(f\"Shape of y_test: {y_test.shape}\")\n\n    # Reshape data for Conv1D and LSTM input (samples, time_steps, features)\n    X_train = np.expand_dims(X_train, axis=-1)\n    X_test = np.expand_dims(X_test, axis=-1)\n\n    # Build the model\n    model = Sequential()\n\n    # Convolutional Layer for feature extraction\n    model.add(Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Dropout(0.5))\n\n    # LSTM Layer to capture sequential information\n    model.add(LSTM(128, return_sequences=True))\n    model.add(LSTM(64))\n    model.add(Dropout(0.5))\n\n    # Fully connected layers\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Early stopping to avoid overfitting\n    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n    # Train the model\n    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stop])\n\n    # Evaluate the model\n    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n    print(f\"Test accuracy: {test_accuracy}\")\nelse:\n    print(\"No valid features or labels to train the model.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T14:53:38.332009Z","iopub.execute_input":"2024-11-08T14:53:38.332421Z","iopub.status.idle":"2024-11-08T14:57:47.953707Z","shell.execute_reply.started":"2024-11-08T14:53:38.332381Z","shell.execute_reply":"2024-11-08T14:57:47.952754Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Shape of y (before encoding): (10120,)\nShape of y_encoded (after encoding): (10120, 2)\nShape of X_train: (6072, 20)\nShape of y_train: (6072, 2)\nShape of X_test: (4048, 20)\nShape of y_test: (4048, 2)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6478 - loss: 0.6535 - val_accuracy: 0.6700 - val_loss: 0.6374\nEpoch 2/50\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6654 - loss: 0.6462 - val_accuracy: 0.6700 - val_loss: 0.6342\nEpoch 3/50\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6623 - loss: 0.6448 - val_accuracy: 0.6700 - val_loss: 0.6372\nEpoch 4/50\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6717 - loss: 0.6372 - val_accuracy: 0.6700 - val_loss: 0.6363\nEpoch 5/50\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6669 - loss: 0.6392 - val_accuracy: 0.6700 - val_loss: 0.6343\nEpoch 6/50\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6695 - loss: 0.6388 - val_accuracy: 0.6700 - val_loss: 0.6350\nEpoch 7/50\n\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6690 - loss: 0.6378 - val_accuracy: 0.6700 - val_loss: 0.6357\n\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6711 - loss: 0.6335\nTest accuracy: 0.6699604988098145\n","output_type":"stream"}]},{"cell_type":"code","source":"from pydub import AudioSegment\nimport numpy as np\nimport librosa\nimport tensorflow as tf\n\n# Load your audio file\naudio_file_path = r'/kaggle/input/hindi-hate-speech-audio-data-wav/Converted_Audio_Data/Converted_Audio_Data/Audio_3009.wav'\naudio = AudioSegment.from_wav(audio_file_path)\n\n# Define the segment duration and the list of timestamps to mute\nsegment_duration = 500  # Duration of each segment in milliseconds (e.g., 500ms)\nbad_word_indices = []  # This will contain the indices of segments predicted as abusive\n\n# Function to extract features from audio segments\ndef extract_features_from_segment(segment):\n    try:\n        # Convert the AudioSegment to numpy array\n        samples = np.array(segment.get_array_of_samples()).astype(np.float32)\n        # Extract MFCCs (same settings as used in training, using 40 MFCCs for example)\n        mfccs = librosa.feature.mfcc(y=samples, sr=segment.frame_rate, n_mfcc=40)  # Use 40 MFCCs\n        return mfccs.T  # Return the MFCCs for each time-step (transposed)\n    except Exception as e:\n        print(f\"Error processing segment: {e}\")\n        return None\n\n# Split the audio into segments and process them\nsegments = []\nfor i in range(0, len(audio), segment_duration):\n    segment = audio[i:i + segment_duration]\n    mfccs = extract_features_from_segment(segment)\n    \n    if mfccs is not None:\n        segments.append(mfccs)\n\n# Convert list of segments to a numpy array and ensure consistent shape by padding/truncating\nmax_sequence_length = 5349  # Define the length that matches your model's expected input size\nX_test = []\n\n# Ensure all MFCCs have the same length (pad or truncate each segment)\nfor mfcc in segments:\n    if mfcc.shape[0] < max_sequence_length:\n        # Padding with zeros if MFCC sequence is shorter than max_sequence_length\n        padded_mfcc = np.pad(mfcc, ((0, max_sequence_length - mfcc.shape[0]), (0, 0)), mode='constant')\n    else:\n        # Truncate if the MFCC sequence is longer than max_sequence_length\n        padded_mfcc = mfcc[:max_sequence_length, :]\n    X_test.append(padded_mfcc)\n\n# Convert the list to a numpy array\nX_test = np.array(X_test)\n\n# Ensure the input has shape (num_samples, sequence_length, num_features)\n# Here we're using 40 MFCC features per time-step\nprint(f\"X_test shape before reshaping: {X_test.shape}\")\n\n# Ensure the input has the shape (batch_size, time_steps, num_features)\n# Here we're using 40 MFCC features per time-step\n# Reshape if necessary\nif X_test.shape[-1] != 40:\n    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 40)  # Ensure it has 40 features per timestep\n\nprint(f\"X_test shape after reshaping: {X_test.shape}\")\n\n# Now let's check the model's expected output shape before flattening\n# If the error persists, check the actual shape of your model output\ntry:\n    # This will give us an idea of the shape before it is passed to the Dense layers\n    intermediate_output = model.layers[0].output  # First layer to check\n    print(f\"Intermediate output shape before Dense layers: {intermediate_output.shape}\")\n\n    # Make predictions on the audio segments\n    predictions = model.predict(X_test)\n\n    # Debugging predictions\n    print(f\"Predictions: {predictions}\")\n\n    for i, pred in enumerate(predictions):\n        print(f\"Segment {i} prediction: {np.argmax(pred)}\")  # Debugging prediction for each segment\n\n        if np.argmax(pred) == 1:  # Check if the segment is predicted as abusive\n            start_time = i * segment_duration  # Start time in milliseconds\n            end_time = start_time + segment_duration  # End time in milliseconds\n            bad_word_indices.append((start_time, end_time))\n    \n    print(f\"Bad word indices: {bad_word_indices}\")  # Debugging the identified bad words\n\n    # Mute the identified segments\n    for start, end in bad_word_indices:\n        audio = audio[:start] + AudioSegment.silent(duration=end - start) + audio[end:]\n\n    # Save the modified audio\n    audio.export(\"muted_audio_for_3009_500ms_audio.wav\", format=\"wav\")\n\nexcept Exception as e:\n    print(f\"Error during prediction: {e}\")\nelse:\n    print(\"No valid segments to predict.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T14:58:33.452637Z","iopub.execute_input":"2024-11-08T14:58:33.452993Z","iopub.status.idle":"2024-11-08T14:58:36.038510Z","shell.execute_reply.started":"2024-11-08T14:58:33.452958Z","shell.execute_reply":"2024-11-08T14:58:36.037517Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"X_test shape before reshaping: (163, 5349, 40)\nX_test shape after reshaping: (163, 5349, 40)\nIntermediate output shape before Dense layers: (None, 18, 64)\nError during prediction: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv1d_28\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (32, 5349, 40)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 5349, 40), dtype=float32)\n  • training=False\n  • mask=None\n","output_type":"stream"}]}]}